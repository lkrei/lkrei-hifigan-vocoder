# Отчёт: HiFi-GAN вокодер для синтеза речи на русском 

Реализован вокодер HiFi-GAN по статье J.Kong et al. (2020). Модель обучена на датасете RUSLAN, логирование через W&B. 

(Визуализации отражены в Experiments-ноутбуке).

**Ссылки:**
- Логи обучения (W&B): [проект hifigan_vocoder](https://wandb.ai/aredaw882-hse-university/hifigan_vocoder)
- [Demo](https://colab.research.google.com/drive/1hfjo5XbylGz5n_f6cv0RPMExKhcBNz4h?usp=sharing) — инференс, MOS-сэмплы, скорость
- [Experiments & Report](https://colab.research.google.com/drive/13soijKUCS1vU8JcAkfDYbQWVTXIBWWyo?usp=sharing) — анализ на RUSLAN, MOS и полный TTS

---

## 1. Выбранная модель и архитектура

Выбрана модель **HiFi-GAN** (Generative Adversarial Networks for Efficient and High Fidelity Speech Synthesis). Архитектура состоит из одного генератора и двух дискриминаторов: Multi-Period (MPD) и Multi-Scale (MSD).

### 1.1 Генератор

Генератор полностью свёрточный. На вход подаётся лог-мел-спектрограмма размерности 80 меловых каналов.

- **Входной слой:** `conv_pre` — Conv1d 80 в 512 каналов, ядро 7, padding 3. Weight normalization.
- **Апсемплинг:** четыре слоя ConvTranspose1d с ядрами и stride [16, 16, 4, 4] (stride = kernel_size // 2). Итоговый апсемплинг по времени: 8×8×2×2 = 256, что соответствует hop_length 256 и переходу от кадров мела к сэмплам waveform при sr=22050.
- **MRF (Multi-Receptive Field):** после каждого апсемплинга идёт блок из трёх параллельных ResBlock с ядрами 3, 7, 11 и дилациями (1,1), (3,1), (5,1). Выходы блоков суммируются и усредняются. В каждом ResBlock два свёрточных слоя с weight norm и residual connection. Это даёт разный рецептивное поле для разных паттернов в речи.
- **Выход:** LeakyReLU, затем `conv_post` (Conv1d в 1 канал, ядро 7), tanh. На выходе waveform.

Используется конфигурация V1 из статьи: hidden 512, ядра апсемплинга [16, 16, 4, 4], ядра ResBlock [3, 7, 11].

### 1.2 Дискриминаторы

**Multi-Period Discriminator (MPD).** Пять под-дискриминаторов с периодами [2, 3, 5, 7, 11] (простые числа, чтобы уменьшить перекрытие). Входной 1D waveform решейпится в 2D тензор (T/p × p), затем обрабатывается 2D свёртками с ядром (k, 1) по оси периода. Так дискриминатор видит периодические паттерны разной длины, что важно для речи. Во всех слоях weight normalization. Каждый под-дискриминатор выдаёт один скаляр и список промежуточных фичей для feature matching.

**Multi-Scale Discriminator (MSD).** Три под-дискриминатора: на сыром аудио, на ×2 average-pooled и на ×4 average-pooled. Цепочка 1D свёрток с группами, первый под-дискриминатор со spectral normalization, остальные с weight normalization. Нужен для последовательных и долгих зависимостей, в отличие от MPD, который смотрит на периодичности.

### 1.3 Функции потерь

- **Adversarial (LS-GAN):** дискриминатор минимизирует (D(real) − 1)² + D(fake)², генератор минимизирует (D(fake) − 1)². Least squares вместо BCE для более стабильных градиентов.
- **Feature matching (λ = 2):** L1 между промежуточными фичами дискриминатора на реальном и сгенерированном аудио по всем под-дискриминаторам MPD и MSD.
- **Mel-spectrogram (λ = 45):** L1 между мел-спектрограммой сгенерированного и мел-спектрограммой исходного аудио. Мел извлекается тем же модулем, что и при обучении.

Итоговая цель для генератора: L_adv + 2·L_fm + 45·L_mel. Для дискриминатора только L_adv. Код лоссов в `src/loss/hifigan_loss.py`:

```python
def discriminator_loss(disc_real_outputs, disc_fake_outputs):
    loss = 0.0
    for dr, df in zip(disc_real_outputs, disc_fake_outputs):
        loss += torch.mean((dr - 1.0) ** 2) + torch.mean(df ** 2)
    return loss

def generator_loss(disc_fake_outputs):
    loss = 0.0
    for df in disc_fake_outputs:
        loss += torch.mean((df - 1.0) ** 2)
    return loss

def feature_matching_loss(disc_real_features, disc_fake_features):
    loss = 0.0
    for real_feats, fake_feats in zip(disc_real_features, disc_fake_features):
        for rf, ff in zip(real_feats, fake_feats):
            loss += F.l1_loss(rf, ff)
    return loss
```

### 1.4 Обработка мел-спектрограмм

Один и тот же код используется при обучении и при инференсе. Конфигурация: sr=22050, n_fft=1024, hop_length=256, win_length=1024, n_mels=80, f_min=0, f_max=11025, power=2. Mel-базис Slaney (librosa). После вычисления спектрограммы: clamp(min=1e-5) и log(). Нормализация не меняется между train и test.

Пример конфигурации и прямого прохода (файл `src/transforms/mel_spectrogram.py`):

```python
@dataclass
class MelSpectrogramConfig:
    sr: int = 22050
    win_length: int = 1024
    hop_length: int = 256
    n_fft: int = 1024
    f_min: int = 0
    f_max: int = 11025
    n_mels: int = 80
    power: float = 2.0
    pad_value: float = -11.5129251

def forward(self, audio: torch.Tensor) -> torch.Tensor:
    mel = self.mel_spectrogram(audio).clamp_(min=1e-5).log_()
    return mel
```

Базис мела задаётся через `librosa.filters.mel` и копируется в `mel_scale.fb` слоя torchaudio, чтобы совпадать с типичным TTS-пайплайном.

---

## 2. Подготовка данных

Использован датасет RUSLAN (мужской диктор, русский язык). Исходная частота 44100 Hz, ресемплинг в 22050 Hz. при стерео усредняется до моно.

Разбиение: 90% train, 5% val, 5% test, seed=42. При обучении из каждого файла вырезается случайный сегмент длиной 8192 сэмпла; на val/test возвращается целый файл (без обрезки). Мел-спектрограммы считаются на лету из аудио тем же MelSpectrogram-модулем, что и в конфиге.

---

## 3. Обучение

Обучение проводилось примерно 13–15 эпох в сумме с разными запусками (часть с нуля, часть с дообучения от сохранённого чекпоинта). Оптимизатор AdamW для G и D: lr=2e-4, betas=(0.8, 0.99), weight_decay=0.01. Схема обучения по эпохам: ExponentialLR с gamma=0.999. Batch size 8. Логи потерь и сэмплы аудио записывались в W&B. Лучшая модель выбиралась по минимальному val_loss_mel, чекпоинты сохранялись каждые 5 эпох в папку results (model_best.pth и checkpoint-epochN.pth).

---

## 4. Воспроизведение модели

Клонирование и установка:

```bash
git clone https://github.com/lkrei/lkrei-hifigan-vocoder.git
cd lkrei-hifigan-vocoder
pip install -r requirements.txt
```

Скачивание чекпоинта:

```bash
pip install gdown
mkdir -p results
gdown "https://drive.google.com/uc?id=1vu9cWf_3ewtkNtC_oi9mJaOt6ms8jgFX" -O results/model_best.pth
```

Обучение с нуля (пример):

```bash
python train.py data_dir=/path/to/RUSLAN/audio
```

Инференс (resynthesis): в папке должна быть подпапка `audio/` с wav-файлами. По умолчанию загружается `results/model_best.pth`.

```bash
python synthesize.py custom_dir_path=wav_test synthesizer.output_dir=output
```

Сгенерированные файлы появятся в `output/` с теми же именами, что и исходные.

---

## 6. Результаты экспериментов

### 6.1 Анализ на обучающих данных (RUSLAN)

Взято 16 аудиозаписей из тестовой части RUSLAN (000016, 000068, 000069, 000070 и др.), что соответствует рекомендации проводить анализ на 10–20 образцах для количественной статистики. Для каждой из исходных аудио извлекли мел-спектрограммы тем же модулем, что и при обучении, подали их в вокодер (resynthesize) и получили сгенерированные версии. Сравнение проведено во временной области (waveform) и в частотно-временной (мел-спектрограммы).

**Какие различия вы видите?** На waveform огибающая, паузы и ударные слоги совпадают с оригиналом; ритм сохранён. Амплитуда сгенерированного сигнала чуть ниже (пики около ±0.5 против ±0.7 у оригинала). На мел-спектрограммах формантные полосы и основные гармоники совпадают; по карте Mel L1 difference ошибки сосредоточены на паузах и в верхних меловых каналах (высокие частоты), вокодер слегка сглаживает детали. Mel L1 error по 16 файлам стабилен (0.43–0.55), выбросов нет.

**Можно ли на слух понять, что аудио синтезировано?** При прослушивании ресинтез звучит разборчиво и естественно; отличия от оригинала в основном в лёгком снижении яркости высоких частот и чуть меньшей громкости. Без прямого A/B сравнения заметить синтез сложно.

**Можно ли это определить по форме волны или спектрограмме?** По waveform при внимательном сравнении видно небольшое занижение амплитуды. По спектрограмме в верхних mel-каналах видно лёгкое сглаживание. То есть по графикам отличия видны, но они небольшие.

**Какие выводы вы можете сделать?** На обучающих данных вокодер хорошо воспроизводит структуру речи; количественно ошибка стабильна и предсказуема.

**Вывод (часть 1).** Resynthesis на данных RUSLAN показывает, что вокодер хорошо воспроизводит общую структуру речи: огибающая waveform совпадает, паузы и ударные слоги на месте, речь разборчива. Mel-спектрограммы оригинала и сгенерированного аудио визуально очень похожи, основные гармоники и формантная структура сохраняются. На mel difference видно, что основные ошибки приходятся на тихие участки (паузы) и на верхние mel-каналы (высокие частоты); вокодер слегка сглаживает высокочастотные детали. Mel L1 error стабильный по файлам (~0.43–0.55), без выбросов.

<img width="1369" height="495" alt="download" src="https://github.com/user-attachments/assets/3b13655f-f549-436c-969a-d96a4aac7c07" />


<img width="1789" height="1181" alt="download-1" src="https://github.com/user-attachments/assets/efdb30ef-244a-4ad9-af31-ff71d544e959" />


<img width="989" height="290" alt="download-2" src="https://github.com/user-attachments/assets/14c75f38-fcee-4a3c-aeae-fda8824cd057" />


### 6.2 Анализ на внешних данных (MOS)

Взяты аудиозаписи тестовых предложений MOS (1.wav, 2.wav, 3.wav). Из них извлечены мел-спектрограммы, сгенерированы синтезированные версии вокодером, проведено сравнение по waveform и мел-спектрограммам.

**Справедливы ли выводы из анализа на обучающих данных?** Частично. Общая картина та же: огибающая и паузы сохраняются, речь разборчива, структура мела в целом воспроизведена. Но на MOS отличия сильнее: больше сглаживание высоких частот, ниже амплитуда, выше Mel L1 error.

**Какие различия между обучающим (RUSLAN) и внешним (MOS) датасетами?** На MOS амплитуда сгенерированного аудио заметно ниже оригинала (на MOS 3 до ±0.5 против ±1.0). Высокие mel-каналы (>60) сглаживаются сильнее. На mel difference ошибка больше, в том числе в средних каналах (MOS 2, MOS 3). Средний Mel L1 error на MOS (~0.92) почти в два раза выше, чем на RUSLAN (~0.49), разброс по MOS больше.

**Причины различий.** Модель обучена только на RUSLAN (один диктор, одна запись). MOS — другие условия: другой диктор/микрофон/громкость. Вокодер хуже генерализует на незнакомые акустические условия, отсюда большая ошибка и более заметные отличия на MOS.

**Вывод (часть 2).** На MOS огибающая и паузы сохраняются, но амплитуда сгенерированного аудио ниже оригинала (на MOS 3 примерно ±0.5 против ±1.0), что заметнее, чем на RUSLAN, и может быть связано с другими акустическими условиями. Mel-структура в целом воспроизведена, но высокие mel-каналы (>60) сглаживаются сильнее. Паттерн ошибок на mel difference похож на RUSLAN, однако на MOS 2 и MOS 3 ошибка больше, в том числе в средних каналах. Mel L1 error на MOS почти в два раза выше, чем на RUSLAN; модель обучена на RUSLAN и хуже генерализует на внешние данные с другими условиями записи.

### 6.3 Анализ полной TTS системы

Использованы текстовые транскрипции: из тестовой части RUSLAN (те же 16 файлов) и из внешнего датасета (три тестовых предложения MOS). Аудио получено пайплайном: акустическая модель MMS-TTS (facebook/mms-tts-rus) по тексту даёт waveform, затем ресемплинг в 22050 Hz, извлечение мела нашим модулем, наш вокодер выдаёт финальное аудио. Для сравнения использованы также варианты только resynthesis (mel из оригинала в вокодер).

**Какие новые артефакты по сравнению с resynthesize?** Изменённый темп речи, другой тембр (MMS-TTS обучена на другом дикторе), менее выразительная интонация, иногда нечёткое произношение отдельных слов. На спектрограммах TTS+vocoder видна другая длительность и ритм по сравнению с оригиналом и с resynthesis.

**Можно ли различить влияние акустической модели и вокодера?** Да. Mel L1 error при полном TTS в 7–8 раз выше, чем при resynthesis (3.5–4.1 на RUSLAN, ~3.9 на MOS против 0.45–0.55 и ~0.9 соответственно). На тройных сравнениях (original / resynthesis / TTS+vocoder) resynthesis визуально близок к оригиналу; mel от TTS+vocoder сильно отличается по длительности, ритму и спектру. При resynthesis ошибка порядка 0.5, разница TTS с оригиналом 3.5–4.0, то есть порядка 85–90% ошибки вносит акустическая модель, а не вокодер.

**Какие ограничения TTS систем обнаружены?** Качество конечного TTS определяется самым слабым звеном; здесь это акустическая модель. Даже хороший вокодер не компенсирует неточный или несовпадающий по просодии mel от AM. Сравнение TTS-выхода с одной конкретной записью по Mel L1 даёт верхнюю оценку ошибки: один и тот же текст можно произнести по-разному, единого «правильного» мела для текста не существует.

**Вывод (часть 3).** Mel L1 error при полном TTS (текст в MMS-TTS, затем mel в наш вокодер) в 7–8 раз выше, чем при resynthesis. На спектрограммах resynthesis близок к оригиналу, а mel от TTS+vocoder сильно отличается по ритму, длительности и спектру. Основной вклад в ошибку вносит акустическая модель; вокодер при ресинтезе даёт ошибку порядка 0.5, а разница TTS с оригиналом 3.5–4.0. Артефакты: другой темп, тембр, интонация. Ограничение: качество TTS определяется самым слабым звеном (здесь AM); сравнение по Mel L1 с одной записью даёт верхнюю границу ошибки, так как правильного мела для текста не существует.

---

## 7. Что сработало и что нет

Сработало: реализация HiFi-GAN по статье с MPD периодами [2,3,5,7,11], MSD и MRF в генераторе; один и тот же mel-фронтенд при обучении и инференсе; LS-GAN + feature matching + mel L1 с коэффициентами 2 и 45; обучение на сегментах 8192 сэмпла; ресемплинг RUSLAN в 22050 Hz; выбор лучшей модели по val_loss_mel и сохранение чекпоинтов. На RUSLAN ресинтез по waveform и мел-спектрограммам близок к оригиналу, ошибка стабильна по файлам.

Не сработало или ограничено: на внешних данных (MOS) ошибка и сглаживание высоких частот заметнее, амплитуда ниже; полный TTS упирается в качество акустической модели, вокодер не компенсирует её ошибки. Ablation и бонусные улучшения (робастность к синтетическому мел, архитектурные доработки) не делались.

---

## 8. Основные сложности

- Подбор и согласование конфигурации мела между датасетом, обучением и инференсом, чтобы не было train-test mismatch.
- Стабилизация обучения GAN (чередование шагов D и G, градиенты, веса лоссов); помогли фиксированные λ_fm и λ_mel из статьи.
- Ограничение по времени: обучение примерно 13–15 эпох с перезапусками, без длительного перебора гиперпараметров и без ablation.
- Для раздела про полную TTS пришлось подобрать внешнюю акустическую модель (MMS-TTS), согласовать частоту дискретизации и способ извлечения мела с нашим вокодером.

---

## 9. Финальные выводы

**Качество синтезированных образцов.** При resynthesis на RUSLAN качество высокое: речь разборчива, огибающая и паузы сохранены, спектральная структура близка к оригиналу. На внешних данных (MOS) качество ниже: заметнее сглаживание высоких частот и занижение амплитуды, но речь по-прежнему разборчива. В полном TTS пайплайне качество упирается в акустическую модель; вокодер лишь преобразует заданный mel в звук.

**Легкость обнаружения синтеза.** На RUSLAN по слуху отличить ресинтез от оригинала без A/B в целом можно, но сложно; по спектрограмме видны небольшие отличия в высоких частотах. На MOS отличия по амплитуде и деталям спектра заметнее. Полный TTS легко отличим по темпу, тембру и интонации от оригинальных записей.

**Ограничения системы.** Вокодер обучен на одном датасете (RUSLAN) и хуже переносит другие акустические условия. В TTS основное ограничение — акустическая модель; вокодер не исправляет её ошибки. Воспроизводимость обеспечена README и ноутбуками; логи и сэмплы доступны в W&B по ссылкам выше.
